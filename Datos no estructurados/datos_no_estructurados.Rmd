---
title: "Introducción a Ciencia de Datos en Salud"
subtitle: "Datos no estructurados"
output: 
  html_document:
    css: style.css 
    fig_height: 8
    fig_width: 12
    toc: true
    toc_float:
      toc_collapsed: true
    toc_depth: 3
    number_sections: true
    theme: paper
editor_options: 
  chunk_output_type: console
---
```{r options, echo = FALSE}

knitr::opts_chunk$set(warning = FALSE,
                      error = FALSE,
                      message = FALSE)

``` 

--------------------------------------------------------------------------------------  

  Ivan Recalde
  
  23-06-2020

---------------------------------------------------------------------------------------


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)

```

# Introducción

## Objetivo

Demos inicio a este tutorial sobre datos no estructurados, donde veremos algunas herramientas básicas para poder extraer información de los mismos. Comencemos!

## Estructurado vs No Estructurado

Cuando hablamos de datos estructurados nos referimos a la información que se suele encontrar en la mayoría de bases de datos. Se suelen mostrar en filas y columnas con títulos. Son datos que pueden ser ordenados y procesados fácilmente por todas las herramientos que solemos usar para el análisis de datos. Lo podríamos ver como si fuese un archivador perfectamente organizado donde todo está identificado, etiquetado y es de fácil acceso.

```{r, echo=FALSE}
tabla_estructurada <- dplyr::tibble(anio=c('2017','2017','2017','2018','2018','2018'), tipo = c('A','B','C','A','B','C'), n = c('45','60','23','32','63','19'))
knitr::kable(head(tabla_estructurada))
```

Es probable que la mayoría estén familiarizadas con este tipo de datos y ya los estén utilizando con eficacia, así que pasemos a ver los datos no estructurados.

Vamos a comenzar estableciendo que los datos estructurados son mucho más fáciles de procesar que los no estructurados, los cuales crean un desafío mayor. Sin embargo, ambos tipos de datos juegan un papel clave en el análisis efectivo de datos.

Aunque parezca increíble, la base de datos con información estructurada de una organización, ni siquiera contiene la mitad de la información que hay disponible en esta. La mayoría de la información relevante para un proyecto se origina en forma no estructurada, principalmente en formato texto.

Los datos no estructurados, generalmente son datos que no tienen estructura interna identificable. Es un conglomerado masivo y desorganizado de varios objetos que no tienen valor hasta que se identifican y almacenan de manera organizada.

Una vez que se organizan, los elementos que conforman su contenido pueden ser buscados y categorizados (al menos hasta cierto punto) para obtener información.

```{r, echo=FALSE}
tabla_valores_no_estructurada <- dplyr::tibble(detalle = c('se realizaron 45 observaciones del tipo A en el anio 2017','se realizaron 60 observaciones del tipo B en el anio 2017','se realizaron 23 observaciones del tipo C en el anio 2017','se realizaron 32 observaciones del tipo A en el anio 2018','se realizaron 63 observaciones del tipo B en el anio 2018','se realizaron 19 observaciones del tipo C en el anio 2018'))
knitr::kable(head(tabla_valores_no_estructurada))

```

Cada dia nos encontramos en un entorno con más datos disponibles para trabajar, y muchos de ellos no los encontramos con su totalidad de campos estructurados, por eso es que tenemos que aceptar el desafío y explorar como hacer un análisis adecuado de estos campos. Hoy vamos a ver una introduccion a tecnicas que nos van a facilitar esta tarea.

## Modelo tidy en datos de texto

Antes de comenzar con las herramientas, esta bueno recordar que el mismísimo Hadley Wickham, nos introduce el concepto de datos en formato Tidy, cuando estos están dispuestos con variables en las columnas y observaciones en las filas, sin que queden ni filas ni columnas con valores en blanco.

```{r, echo=FALSE}
tabla_estructurada <- dplyr::tibble(anio=c('2017','2017','2017','2018','2018','2018'), tipo = c('A','B','C','A','B','C'), n = c('45','60','23','32','63','19'))
knitr::kable(head(tabla_estructurada))
```

Esto nos permite trabajar de manera eficiente y fácilmente acomodable a las funciones de... Sí, tidyverse! Nos permite saber que cada fila va a ser una observación y que no vamos a tener una tabla con infinitas columnas. Si bien esto es excelente, el problema es que muchas veces los datos de entrada a nuestros scripts/algoritmos no vienen en  tidy, sino que los encontramos de la siguiente manera.

```{r untidy example, echo=FALSE}
tabla_no_tidy <- dplyr::tibble(anio=c('2017','2018'), tipo_a = c('45','32'), tipo_b = c('60','63'),
                               tipo_c=c('23','19'))

knitr::kable(head(tabla_no_tidy))
```

Este modelo de representación de los datos no es peor ni mejor, como se daran cuenta suele a veces hasta ser más rápido para visualizar observaciones en un data frame.

En paralelo tal y como el modelo tidy de los datos nos permite manejar de manera más fácil y efectiva los datos, lo mismo pasa con los datos de texto. En donde se busca que cada registro tenga uno y solo un token. Cuando hablamos de token nos referimos a lo que para nosotros es una unidad significativa de texto, en lo que nos queremos concentrar, en la mayoría de los casos básicos vamos a hablar de token como palabras individuales pero es importante saber que cuando el análisis es más complejo podríamos buscar que nuestros token sean frases o párrafos enteros e intentar identificar significado estos. Pero como pasaba con los datos estructurados normalmente no los encontramos en formato tidy, por eso una primer herramienta que vamos a ver es la 'Tokenizacion' de estos campos de texto.

# Tokenizar

Arranquemos! Vamos a trabajar con un bello poema de Tamara Grosso

```{r}
# Tamara Grosso @tamaraestaloca cuando todo refugio se vuelve hostil @santoslocospoesia
poema <- c('ADVERTENCIA:',
                 'No se decirte',
                 'si todo va a mejorar',
                 'pero seguro la ficha',
                 'que te hizo ser quien sos',
                 'te cayo despues',
                 'de uno de los peores',
                 'dias de tu vida')
```

En este caso tenemos un vector de datos en formato character, un primer paso útil sería pasarlo a un data frame, de esta manera lo traemos a un formato con el que estamos más acostumbradxs a trabajar. Para esto vamos a disponibilizar todas las funciones de Tidyverse y utilizar la función tibble(), que nos permite la creacion de dataframes, pasandole los nombres de las columnas igualados a vectores con los valores. Le agregamos una columna extra para no perder el numero de linea.

```{r pressure}
library(tidyverse)

poema_df <- tibble(linea = 1:8, texto = poema)

poema_df
```

Ahora estamos frente a uno de los casos más comunes, un data frame donde una columna tiene texto libre. Vamos entonces a proceder a utilizar una función para tokenizar nuestro texto. En este caso el análisis lo podriamos pensar sobre palabras por separado, que en principio podrían ser nuestra unidad significativa de texto. ¿Podríamos también utilizar los versos?
La función que vamos a usar es unnest_tokens() de la biblioteca 'tidytext'. Su uso más simple es usar los pipes de magrittr para pasarle el data frame como primer parámetro implícito, luego el nombre que queremos que la columna de tokens tenga y por último el nombre de la columna de origen donde debería buscar el texto a tokenizar.

```{r}
library(tidytext)

texto_tokenizado <- poema_df %>%
  unnest_tokens(palabra_poema,texto)

texto_tokenizado

```

Veamos poner la lupa en un par de cositas bellas que nos dejó la función. En principio vemos que cada palabra quedó en una fila, ¿estaríamos ahora en condiciones de afirmar que cada observación está contenida en un registro diferente? Luego vemos que para facilitar el manejo nos transformó todos los tokens a minúsculas; en el caso de no querer esto, podemos pasarle a la funcion como parametro de entrada, to_lower = FALSE de la siguiente manera.

```{r}
poema_df %>%
  unnest_tokens(palabra_poema,texto, to_lower = FALSE)
```

## Completemos un circuito basico de analisis y armemos unas visualizaciones

Vamos entonces a imaginar que tenemos varios textos consecutivos (poemas en nuestro caso). Para hacer un poco mas divertido el análisis. 

```{r, include=FALSE}
poema2 <- c('ENTRE NOSOTROS:',
                    'Quisiera saber si alguna vez',
                    'se van a poder leer las mentes',
                    'Para averiguar lo que de verdad pasó entre nosotros',
                    'En tu versión del desamor')
poema2_df <- tibble(linea = 1:5, texto = poema2)


poema3 <- c('LOOP:',
            'Todavía me parece',
            'Que vas a venir un día',
            'Y me vas a decir lo que ya no quiero')
poema3_df <- tibble(linea = 1:4, texto = poema3)

poema4 <- c('VARIACIONES SOBRE LA TRISTEZA:',
            'Meter la mano en el cajón de las aspirinas',
            'y sólo encontrar el blister vacío.',
            'Un nuevo descubrimiento',
            'ni vos mismo, en el pasado,',
            'te interesaste por lo que te pasa ahora.')
poema4_df <- tibble(linea = 1:6, texto = poema4)


```

Utilizaremos la función bind_rows para unir los registros de distintos data frames.

```{r}
varios_poemas <- poema_df %>% 
    bind_rows(poema2_df, poema3_df, poema4_df)
```



```{r, echo=FALSE}
varios_poemas <- varios_poemas %>% 
  select(-linea)
varios_poemas
```


Vamos de nuevo a tokenizar este df, como ya habíamos visto anteriormente y vamos a proceder a armar una visualización. Vamos a usar count() para que nos cuente cuantas ocurrencia de cada token hay, simplemente debemos decirle en qué columna se encuentra. Luego nos quedaremos solo con las palabras que aparecieron más de una vez. Por último usamos ggplot, recordamos que este paquete ya se encuentra disponible ya que se encuentra dentro de tidyverse. Usamos tambien, la funcion reorder(), para que luego el gráfico nos muestra las barras ordenadas.

```{r}
varios_poemas_tokenizados <- varios_poemas %>% 
    unnest_tokens(palabra_poema,texto)

varios_poemas_tokenizados%>%
    count(palabra_poema) %>%
    filter(n > 1) %>% 
    ggplot(aes(reorder(palabra_poema, n), n)) +
    geom_col() +
    coord_flip()
```

Otra biblioteca bastante útil para visualizar de manera rápida ocurrencia de tokens (palabras), es wordcloud. Vamos a usar la función de base with(), para poder aplicarlo en nuestro formato con pipes de magrittr. Tenemos que tener cuidado que por defecto la mínima frecuencia de aparición es 3, así que la cambiamos a 0.

```{r}
library(wordcloud)

varios_poemas_tokenizados %>%
  # filter(!(palabra_poema %in% stopwords::stopwords(language = 'spanish'))) %>% 
  count(palabra_poema) %>%
  with(wordcloud(palabra_poema, n,min.freq = 0))
```

Como vemos hay una línea comentada, que filtra todas las palabras que se encuentren dentro de stop_words, ¿que es stop_words?

```{r, echo=FALSE}
stopwords::stopwords(language = 'spanish')[1:25]
```

Es un vector de palabras típicas usadas en algun lenguaje que le pasemos por parámetro de entrada, pero que no aportan significado al texto en la mayoría de las ocasiones. Esto sirve para que las palabras con más ocurrencias no sean siempre las mismas sino que sean palabras significativas que aporten valor del mensaje. No lo usamos porque nuestro ejemplo tenía una cantidad muy baja de palabras y haberlo usado hubiese eliminado casi todas las palabras con más de una ocurrencia como vemos abajo. Lo importante de todas formas es tener presente la existencia de estas colecciones de palabras.

```{r, warning=FALSE}
varios_poemas_tokenizados %>%
  filter(!palabra_poema %in% stopwords::stopwords(language = 'spanish')) %>%
  count(palabra_poema) %>%
  with(wordcloud(palabra_poema, n,min.freq = 0))
```

Una ultima cuestion que está bueno a tener en cuenta es que, si bien stopwords nos aporta una bolsa de palabras que a priori no nos sirven, podemos hacer lo mismo con algunas palabras que nosotros sepamos que para nuestro dominio de datos tampoco es relevante. Probablemente para el análisis de poemas, la palabra 'paciente', es relevante del significado del verso/poema; pero si estamos leyendo evoluciones médicas, lo más probable es que casi todas tengan la palabra paciente. Veamos cómo quedaría un ejemplo donde además le filtró otra bolsa de palabras.

```{r, warning=FALSE}
palabras_varias <- c('si','vas')

varios_poemas_tokenizados %>%
  filter(!palabra_poema %in% stopwords::stopwords(language = 'spanish')) %>%
  filter(!palabra_poema %in% palabras_varias) %>%
  count(palabra_poema) %>%
  with(wordcloud(palabra_poema, n,min.freq = 0))
```

De todas formas podríamos generar un único vector también, para que nuestro código quede más claro.

```{r, warning=FALSE}
palabras_varias <- c('si','vas')
stop_words_spanish <- stopwords::stopwords(language = 'spanish')
palabras_comunes <- c(stop_words_spanish, palabras_varias)

varios_poemas_tokenizados %>%
  filter(!palabra_poema %in% palabras_comunes) %>%
  count(palabra_poema) %>%
  with(wordcloud(palabra_poema, n,min.freq = 0))
```

# Stringr

Vamos a presentar otra herramienta muy potente Stringr. Este paquete nos proporciona un conjunto de funciones para recuperar de manera sencilla información del texto. Esta está construida sobre stringi, otra biblioteca más extensa. Para explotar más su uso y si se quedan con ganas, siempre esta bueno explorar el cheatsheet que tiene.

https://rstudio.com/resources/cheatsheets/

Vamos entonces a echar un vistazo por algunas funciones. Comencemos con algo simple, contemos cuánto caracteres tiene cada verso.

```{r}
library(stringr)

poema_df %>% 
    mutate(cantidad_caracteres = str_count(texto)) 
```

Quedémonos ahora solo con una parte del texto, en este caso los primeros 5 caracteres. Veamos que a nivel gráfico nos muestra el resultado con comillas para denotar que quedó un espacio [' '], al principio o al final.

```{r}
poema_df %>% 
    mutate(solo_primeros_cinco = str_sub(texto,1,5)) 
```

Las posiciones son relativas al largo del texto, podemos entonces decirle que agarre los últimos 5 caracteres de la siguiente manera

```{r}
poema_df %>% 
    mutate(solo_primeros_cinco = str_sub(texto,-5,-1)) 
```

Otro uso típico es querer modificar todo a minúsculas, pero como sobre gustos no hay nada definido también nos permite modificar todo a mayúsculas.

```{r}
poema_df %>% 
    mutate(mayusculas = str_to_upper(texto),
           minusculas = str_to_lower(texto)) 
```

## str_detect()

Veamos ahora cómo identificar patrones dentro del texto libre desde su manera más sencilla y veamos algunos ejemplos que pueden servir como disparadores. 
Generemos una columna nueva que nos diga si este patrón estaba en el texto de cada registro.

```{r}
poema_df %>% 
    mutate(tengo_de = str_detect(texto, 'de')) 
```

Podriamos tambien querer filtrar y quedarnos solo con las ocurrencias de este patrón.

```{r}
poema_df %>% 
    filter(str_detect(texto, 'de')) 
```

Volvamos ahora al caso donde teníamos todos los poemas juntos, ¿sería posible identificar de alguna manera cada poema por separado?
```{r, echo=FALSE}
varios_poemas
```

Busquemos entonces generar un corte cada vez que encuentre los ':', que es en este caso por lo menos lo que nos indica que hay un título. Una función que nos podría servir es cumsum(), que nos va a mantener un contador cada vez que se cumpla una condición que le pasemos por parámetro.

```{r}

poemas_separados <- varios_poemas %>%
  mutate(poema = cumsum(str_detect(texto, ':')))

poemas_separados
```

Excelente, esto nos permite abstraernos de la cantidad de registros que tengamos tokenizados por cada archivo de texto original.

## ¿Cómo recuperar lo que partimos? [¿lo rompi?]

Una vez que aprendimos a separar [romper] algo, estaria buenisimo tambien saber volverlo a armar, ¿verdad?
Vamos a usar la función paste() y le vamos a pasar por parámetro collapse, para definir qué queremos que nos deje en el medio de cada verso del poema en este caso.

```{r}
poemas_unidos <- poemas_separados %>% 
    group_by(poema) %>% 
    mutate(poema_entero = paste(texto,
                           collapse = ' ')) %>% 
    slice(1) %>% 
    ungroup() %>% 
    select(poema_entero)

poemas_unidos
```

## Próximos pasos -> str_extract() y regex()

Para ir finalizando con estas herramientas básicas, volvamos al ejemplo original que usamos para ver cómo podíamos encontrarnos la información no estructurada

```{r, echo=FALSE}
knitr::kable(head(tabla_valores_no_estructurada))

```

Vamos a usar str_extract para obtener la información que esta perdida dentro del campo libre, pero para que esta función realmente explote su potencial va a necesitar que le agreguemos expresiones regulares. Las expresiones pueden ir desde algo muy simple, hasta algo super complejo, podemos ayudarnos del cheatsheet y de paginas como https://regex101.com/ que nos permiten en tiempo real ir probando nuestras expresiones. Vamos a ver como a priori este caso se resuelve con expresiones bastante amigables

```{r}
tabla_valores_no_estructurada %>% 
    mutate(n = str_extract(detalle, regex('[0-9]+')), #uno o mas numeros
           tipo = str_extract(detalle, regex('[ABC] ')), #letras en mayuscula A, B o C
           anio = str_extract(detalle, regex('[0-9]+$'))) #uno o mas numeros y fin de texto
```

Algo interesante a tener en cuenta es observar como la primera expresión regular no trae el anio además del n. Esto se debe a que salvo que le indiquemos lo contrario str_extract(), nos trae solo lo primero que encuentra. Entonces una vez que encuentra uno o más números seguidos, deja de mirar el texto.

# Conclusión

Este pequeño tutorial tiene como finalidad presentar un vistazo rápido por bastantes herramientas para el análisis de texto. ¡Ojalá sirva como disparador para investigar más!

# Bonus

Dado que es una función que es bastante útil y que hay imágen disponible de Allison Horst, por último veremos la función str_squish(), también del paquete stringr.

![](str_squish.png "Logo Title Text 1")

Esta nos permite de manera muy simple eliminar los espacios al principio y al final de nuestro carácter y además si encuentra dos espacios juntos elimina uno. Es una gran herramienta para estandarizar nuestros tokens. Si recuerdan en un momento del taller habíamos llegado al siguiente data frame.

```{r}
poema_df %>% 
    mutate(solo_primeros_cinco = str_sub(texto,1,5)) 
```

Si le aplicamos esta nueva función obtenemos el siguiente resultado. 

```{r}
poema_df %>% 
  mutate(solo_primeros_cinco = str_sub(texto,1,5)) %>% 
  mutate(solo_primeros_cinco = str_squish(solo_primeros_cinco))
```

Cuando trabajamos con tokens o texto que no estamos seguros si esta estandarizado, una práctica que nos puede ahorrar varios dolores de cabeza es a nuestro texto libre, modificarlo y pasarle str_squish y str_to_lower()

```{r}
poema_df %>% 
  mutate(solo_primeros_cinco = str_sub(texto,1,5)) %>% 
  mutate(solo_primeros_cinco = str_to_lower(str_squish(solo_primeros_cinco)))
```


Mas ilustraciones, todas sobre paquetes y funciones, de la genial Allison Horst (@allison_horst en Twitter) las pueden explorar en el siguiente link:

https://github.com/allisonhorst/stats-illustrations


